#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext("*.pdf",
docvarsfrom = "filenames",
docvarnames = c("author", "id"),
sep = "-")
library(quanteda)
library(quanteda.textplots)
library(quanteda.textmodels)
library(quanteda.textstats)
library(readtext)
library(tidyverse)
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext("*.pdf",
docvarsfrom = "filenames",
docvarnames = c("author", "id"),
sep = "-")
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext("*.pdf",
docvarsfrom = "filenames",
docvarnames = c("author", "id"),
sep = "-")
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext("*.pdf",
docvarsfrom = "filenames",
docvarnames = c("author", "id"),
sep = "_")
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext("*.pdf",
docvarsfrom = "filenames",
docvarnames = c("author", "id"),
sep = "_")
#make a corpus
corpus_SV <- read.csv("data_SV.txt")  %>% corpus(.)
#make a corpus
corpus_SV <- data_SV  %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_agenda, pattern = stopwords("no"), selection = "remove")
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("no"), selection = "remove")
print(tokens_SV_nostop)
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext(encoding = "LATIN1", "*.pdf",
docvarsfrom = "filenames",
docvarnames = c("author", "id"),
sep = "_")
#make a corpus
corpus_SV <- data_SV  %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("no"), selection = "remove")
print(tokens_SV_nostop)
url <- c(
"http://www.nsd.uib.no/polsys/data/filer/parti/H3.html", # Høyre 2001
"http://www.nsd.uib.no/polsys/data/filer/parti/H19.html", # AP 2001
"http://www.nsd.uib.no/polsys/data/filer/parti/H25.html", # Venstre 2001
"http://www.nsd.uib.no/polsys/data/filer/parti/H32.html", # SV 2001
"http://www.nsd.uib.no/polsys/data/filer/parti/H34.html", # Krf 2001
"http://www.nsd.uib.no/polsys/data/filer/parti/H37.html", # Senterpartiet 2001
"http://www.nsd.uib.no/polsys/data/filer/parti/H5181.html", # FrP 2001
"http://www.nsd.uib.no/polsys/data/filer/parti/H9364.html", # Høyre 2005
"http://www.nsd.uib.no/polsys/data/filer/parti/H9191.html", # Ap 2005
"http://www.nsd.uib.no/polsys/data/filer/parti/H9362.html", # Venstre 2005
"http://www.nsd.uib.no/polsys/data/filer/parti/H9360.html", # SV 2005
"http://www.nsd.uib.no/polsys/data/filer/parti/H9365.html", # Krf 2005
"http://www.nsd.uib.no/polsys/data/filer/parti/H9366.html", # Senterpartiet 2005
"http://www.nsd.uib.no/polsys/data/filer/parti/H9368.html" # FrP 2005
)
partinavn <- c("H","Ap","V","SV","KrF","Sp","FrP","H","Ap","V","SV","KrF","Sp","FrP")
year <- c(rep(2001,7),rep(2005,7))
url %>%
readtext(encoding = "LATIN1") %>%
corpus() -> valgprogram
docvars(valgprogram, "parti") <- partinavn
docvars(valgprogram, "year") <- year
docvars(valgprogram, "navn") <- paste(partinavn,year, sep ="_")
save(valgprogram, file = "valgprogram.RData")
token_pp <- tokens(
corpus,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
token_pp <- tokens(
valgprogram,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_pp_nostop <- tokens_select(token_pp, pattern = stopwords("no"), selection = "remove")
print(tokens_SV_nostop)
print(tokens_pp_nostop)
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext(encoding = "LATIN1", "*.epub",
docvarsfrom = "filenames",
docvarnames = c("author", "id"),
sep = "-")
#make a corpus
corpus_SV <- data_SV  %>% corpus(.)
install.packages("epubr")
library(epubr)
?epub
data_SV <- epub("*.epub")
data_SV <- epub("C:/Users/HW van der Waal/projects/SV/De bruid, de hoer en de eindtijd - Sigurd Bratlie.epub")
#make a corpus
corpus_SV <- data_SV  %>% corpus(.)
data_SV
data_SV <- epub_unzip("C:/Users/HW van der Waal/projects/SV/De bruid, de hoer en de eindtijd - Sigurd Bratlie.epub")
data_SV
#make a corpus
corpus_SV <- data_SV  %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("no"), selection = "remove")
print(tokens_SV_nostop)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("nl"), selection = "remove")
print(tokens_SV_nostop)
sr_dfm <- dfm(tokens_SV_nostop)
sr_dfm
topfeatures(sr_dfm, 20)
dfm_trim(sr_dfm, max_termfreq = 0.5, termfreq_type = "quantile" )
head(dfm_trim,20)
dfm_trim(sr_dfm, max_termfreq = 0.5, termfreq_type = "quantile" )
head(dfm_trim,20)
dfm_trim
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- read.csv("*.txt")
?read.csv
data_SV <- epub("C:/Users/HW van der Waal/projects/SV/De bruid, de hoer en de eindtijd - Sigurd Bratlie.epub")
data_SV$data
?epub
data_SV <- epub("C:/Users/HW van der Waal/projects/SV/De bruid, de hoer en de eindtijd - Sigurd Bratlie.epub", encoding = "UTF-8")
data_SV$data
list.files(SV, recursive = TRUE)
list.files(., recursive = TRUE)
?list.files
list.files(~, recursive = TRUE)
list.files(path = ".", pattern = "*.epub", recursive = TRUE)
files <- list.files(path = ".", pattern = "*.epub", recursive = TRUE)
epub(files)
data_SV <- epub(files)
corpus_SV <- data_SV  %>% corpus(.)
corpus_SV <- data_SV$data %>% corpus(.)
corpus_SV <- data_SV %>% corpus(.)
?corpus
data_SV <- epub(files) %>% mutate(text_field = data$text, docid_field = title)
corpus_SV <- data_SV %>% corpus(.)
data_SV <- epub(files) %>% mutate(text_field = data, docid_field = title)
corpus_SV <- data_SV %>% corpus(.)
data_SV_text <- unlist(data_SV)
corpus_SV <- data_SV_text %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("nl"), selection = "remove")
print(tokens_SV_nostop)
library(tm)
df <- tm::VCorpus(tm::VectorSource(data_SV))
corpus_SV <- df %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("nl"), selection = "remove")
print(tokens_SV_nostop)
df <- tm::VCorpus(tm::VectorSource(data_SV$data))
corpus_SV <- df %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("nl"), selection = "remove")
print(tokens_SV_nostop)
df <- tm::VCorpus(tm::VectorSource(data_SV$data$text))
corpus_SV <- df %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("nl"), selection = "remove")
print(tokens_SV_nostop)
unlist(data_SV$data)
df <- unlist(data_SV$data)
df
df(head, 5)
corpus_SV <- df %>% corpus(.)
df <- Corpus(data_SV$data)
str(data_SV)
df <- data.frame(author = data_SV$creator)
df <- data.frame(author = data_SV$creator, docid_field = tibble::rowid_to_column(data_SV, "ID"),
text_field = data$text)
df <- as.vector(data$text)
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- read.csv("*.txt")
?readtext
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext("*.txt",
docvarsfrom = "filenames",
docvarnames = c("title", "author"),
dvsep = "-")
corpus_SV <- data_SV %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("en"), selection = "remove")
print(tokens_SV_nostop)
sr_dfm <- dfm(tokens_SV_nostop)
sr_dfm
dfm_trim(sr_dfm, max_termfreq = 0.95, termfreq_type = "quantile" )
head(dfm_trim,20)
set.seed(1, sample.kind = "Rounding")
textplot_network(sr_dfm, omit_isolated = FALSE,
edge_color = "#1F78B4",
edge_alpha = 0.5,
edge_size = 2,
vertex_color = "#4D4D4D",
vertex_size = 2,
vertex_labelcolor = NULL,
vertex_labelfont = NULL,
vertex_labelsize = 5,
offset = NULL,
)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textmodels)
library(quanteda.textstats)
library(tm)
library(readtext)
library(tidyverse)
#in this way we can read reports in pdf and extract document variables from the filenames
#the filenames need all to be in exactly the right format
data_SV <- readtext("*.txt",
docvarsfrom = "filenames",
docvarnames = c("title", "author"),
dvsep = "-")
corpus_SV <- data_SV %>% corpus(.)
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("en"), selection = "remove")
print(tokens_SV_nostop)
dfm(corpus_SV) %>%
tidy() %>%
rename("n" = count) -> tmp
library(broom)
dfm(corpus_SV) %>%
tidy() %>%
rename("n" = count) -> tmp
token_SV <- tokens(
corpus_SV,
split_hyphens = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
include_docvars = TRUE
)
tokens_SV_nostop <- tokens_select(token_SV, pattern = stopwords("en"), selection = "remove")
print(tokens_SV_nostop)
dfm(tokens_SV_nostop) %>%
tidy() %>%
rename("n" = count) -> tmp
tmp %>%
group_by(document) %>%
summarise(lengde = sum(n)) %>%
left_join(tmp) -> bok_ord
bok_ord %>%
ggplot(aes(term, tf_idf, fill = document)) + # lag figur
geom_col(show.legend = FALSE) + # stolpediagram
facet_wrap(~parti, ncol =2, scale = "free") + # ett per parti
theme(axis.title.x=element_blank(), # ta bort info å aksene
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y = element_blank()) +
coord_flip() -> fig_unfixed
fig_unfixed
bok_ord %>%
ggplot(aes(term, tf_idf, fill = document)) + # lag figur
geom_col(show.legend = FALSE) + # stolpediagram
facet_wrap(~document, ncol =2, scale = "free") + # ett per parti
theme(axis.title.x=element_blank(), # ta bort info å aksene
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y = element_blank()) +
coord_flip() -> fig_unfixed
fig_unfixed
?tf_idf
tstat_freq <- textstat_frequency(bok_ord, n = 5, groups = lang)
tstat_freq <- textstat_frequency(dfm(tokens_SV_nostop), n = 5, groups = lang)
tstat_freq <- textstat_frequency(dfm(tokens_SV_nostop), n = 5)
head(tstat_freq, 20)
tstat_freq
dfmmat <- dfm(tokens_SV_nostop)
tstat_freq <- textstat_frequency(dfmmat)
tstat_freq
tstat_freq <- textstat_frequency(dfmmat, groups = author)
tstat_freq
dfmat %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
dfmmat %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
dfmmat %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal() +
facet_grid(. ~ author)
dfmmat %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal() +
facet_wrap(. ~ author)
tstat_freq %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal() +
facet_wrap(. ~ author)
tstat_freq %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal() +
facet_wrap(. ~ group)
tstat_freq <- textstat_frequency(dfmmat, n = 100, groups = author)
tstat_freq %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal() +
facet_wrap(. ~ group)
tstat_freq <- textstat_frequency(dfmmat, n = 50, groups = author)
tstat_freq %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal() +
facet_wrap(. ~ group)
tstat_freq %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_classic() +
facet_wrap(. ~ group)
tstat_freq %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_linedraw() +
facet_wrap(. ~ group)
